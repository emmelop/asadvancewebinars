{
	"name": "05_CREATE_SALES_BY_PARTITION",
	"properties": {
		"folder": {
			"name": "W01-ASyn4-BIBA_Roles/Demo 02 Partitioning data into ADLSg2"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 6,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "6",
				"spark.dynamicAllocation.maxExecutors": "6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ad80bb35-cd7e-414e-84dc-af93d138fef0/resourceGroups/rg-demosynapse/providers/Microsoft.Synapse/workspaces/syn-asa/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://syn-asa.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Sales_opt partition by Period Year and Month - only 2011, \n",
					"import datetime\n",
					"from pyspark.sql.functions import year, month, concat, lpad\n",
					"from pyspark.sql import SparkSession\n",
					"from datetime import date, timedelta\n",
					"from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
					"\n",
					"read_path = 'abfss://tempdata@dlsasa.dfs.core.windows.net/curated/Sales/*.parquet'\n",
					"write_path = 'abfss://raw@dlsasa.dfs.core.windows.net/curated/Sales_opt/'\n",
					"\n",
					"df = spark.read.load(read_path, format='parquet')\n",
					"\n",
					"df = df.withColumn(\"Year\", year(\"Date\"))\n",
					"df = df.withColumn(\"Month\", lpad(month(\"Date\"),2,'0'))\n",
					"#print(df.rdd.getNumPartitions())\n",
					"\n",
					"df =df.coalesce(1)\n",
					"#print(df.rdd.getNumPartitions())\n",
					"dfp = df.where(year(\"Date\")==2011)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"# 2012 23 minutos / 173 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2012)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"# 2013 8 minutos / 37 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2013)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# 2014 23 minutos / 134 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2014)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"# 2015 31 minutos / 68 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2015)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# 2016 16 minutos / 90 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2016)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"# 2017 25 minutos / 60 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2017)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"# 2018 17 minutos / 144 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2018)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"# 2019 34 minutos / 131 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2019)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"# 2020 40 minutos / 2 Mill 6 medium nodes\n",
					"dfp = df.where(year(\"Date\")==2020)\n",
					"dfp.write.mode(\"append\").partitionBy(\"Year\",\"Month\").parquet(write_path)"
				],
				"execution_count": 10
			}
		]
	}
}